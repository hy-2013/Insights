---
{"dg-publish":true,"permalink":"/030 PKV/AI/基础/【ML基础】从特征组合及其权重的角度对常用模型做统一解释 - 2022H1/"}
---

## 总思路

如果把机器学习理解为**对输入x的组合空间的表征**，那该表征主要包括「特征组合方式」和「其对应权重（weight）」两个核心要素。另外，表示特征方式包括：原始特征值（x_i, x_j）和特征向量embvec（e_i, e_j），e_i*e_j表示e_i和e_j的dot-product。

下面为基于上述两个核心要素的算法对比：

1. 树模型/gbdt：每棵树的leaf node对应的预测结果（response），可以理解为从根节点到该leaf node的路径上所有特征的交叉的weight。

2. mlp/lr：不具有直接原始特征值交叉的能力（所以需要人工交叉），但可以理解为通过线性加权（weight）的方式做全部特征的“弱”交叉，需要使用非线性激活函数来解决非线性问题。

3. fm/ffm：直接的二阶原始特征值交叉（x_i_*x_j_），_weight_为_e_i*_e_j，即由特征向量embvec内积得到，和self-attention一样(q_i*k_j)。fm也可称为二阶特征向量embvec的交叉（x_i, x_j一般为1，可忽略）。

4. transformer（self-attention)：token/特征e_i的交互方式为softmax(W_q_*e_i * W_k**e**{1…n}) * W_v*e__{1…n}。与fm相比都是基于特征向量，但transformer除了有和fm一样的q_i**k_j_，又做了_softmax_归一化和对_v_i_的_weighted sum_（_fm_没有做_softmax_且_v_j_换成_x_i*_x_j）；而且transformer是多层+ffn（获取更深、泛化性更好的交互），fm为一层。
	1. 适用场景：组合空间大 且 链接稠密（共现性高）、高质量数据多
	2. 一层transformer也是二阶交叉，多层则“变相”实现了多阶交叉。cnn、gnn也类似，rnn相当于横向多层
	3. 替换统计特征的难点：1.模型训练batch小，而统计特征是基于全局的统计。2.一些组合特征的共现性低（比如个性化特征），容易过拟合，并会被共现性高的特征影响，而用统计的方式很容易做

5. cnn：image每个pixel/特征的embvec为feature map的channel维（即e_i, e_j）。filter卷积操作即为weighted e_i和e_j（而self-attention的weight来自e_i*e_j，且cnn非全局而是local的）。

6. ViT：因为做pixel级embvec的计算量和存储太大，改为一个patch做为一个embvec，然后用self-attention

7. lstm（假设激活函数为relu和不考虑bias）：e_t和e__{t-1}*的交叉部分类似_W_f*W_i*e_t*e**{t-1}和W_i**W_h**e_t*e*{t-1}，W_f为forget门参数，W_i为输入门参数，W_h为h_t的参数。

8. gat/gnn：类似cnn的filter的field，基于特定node（e_i）的neighbor_node做weighted/pooling e^{i}_{1…n}。

9. DCN：construct all the cross terms_** **_x_1^α1 * x_2^α2 * . . . * x_d^αd（alpha次幂原始特征值的交叉，非特征向量或embvec交叉） with degree |α|>2。理论上基于泰勒公式的幂级数可以拟合任何数据分布或函数，该方法理论基础很好。

### 讨论环节

问题1：x_i和x_j的组合空间很稀疏，为什么fm是有效的？

	1. 对特征x_i在样本中出现的频次做过滤，保证每个embvec均可训练。
	2. 只会通过 样本中出现的特征组合 对 相关的特征表征embvec做BP训练。（如果样本存在模式坍塌或非完备的特征组合分布，会容易过拟合，所以这类方法对样本量的要求远大于基于原始特征值的方案，如lr等）。
	3. 特征表征交叉的方式需要训练的参数量远小于「原始特征值笛卡尔积交叉」的方式，从N^2降到N*k，N为特征field的个数。

欢迎补充。希望基于这种串联加深对各算法的理解和作为平时算法选型的参考。
